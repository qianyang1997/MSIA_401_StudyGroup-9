---
title: "HW4MSIA401"
author: "Qiaozhen Wu, Qiana Yang, Dian Yu, Yijun Wu"
date: "10/18/2020"
output:
  html_document: default
---

## R Markdown

## 4.10
```{r}
library(car)
library(dplyr)
data <- c(8,1,1,1,
          8,1,1,0,
          8,1,1,0,
          0,0,9,1,
          0,0,9,1,
          0,0,9,1,
          2,7,0,1,
          2,7,0,1,
          2,7,0,1,
          0,0,0,10,
          0,0,0,10,
          0,0,0,10)
webster <- data.frame(cbind(matrix(data, nrow = 12, ncol = 4, byrow = TRUE), rep(10, 12)))
colnames(webster) <- c("x1", "x2","x3", "x4", "y")
```

### (a)
```{r}
cor(webster[, -5])
```
### No correlation coefficent exceed 0.5 in absolute value. Therefore, correlations do no indicate multilinearity.

### (b)
```{r message=FALSE, warning=FALSE}
fit <- lm(y ~., data = webster)
vif(fit)
```
### All vif exceed 150 and the greatest is 289.375. Therefore, vifs indicate multilinearity. 

## 4.11
### (a)
```{r }
mpg<- read.csv(here::here("mpg.csv"))
cor(mpg)
```
### Yes there is multicollinearity since there are values like 0.95 and 0.8 which are all over 0.7.

###(b)
``` {r}
fit_5 <- lm(mpg ~ cylinders+displacement+horsepower+weight + acceleration , data = mpg)
summary (fit_5)
```
### Multicollinearity is reflected in the fact that three of the variables - cylinders, displacement and acceleration- all appears to be insignificant-- overall F statistic is significant, but 3 out of 5 of the t values are nonsignificant.

### (c)
``` {r}
fit_4 <- lm(mpg ~ cylinders+horsepower+weight + acceleration , data = mpg)
summary (fit_4)
```
### It decreases the value of cylinders and makes it more significant. And the change increased ajusted r squated slightly 

### (d)
``` {r}
plot(fit_4,2)
plot(fit_4,1)
vif (fit_4)
fit_inverse <- lm((100/mpg) ~ cylinders+horsepower+weight + acceleration , data = mpg)
plot(fit_inverse,2)
plot(fit_inverse,1)
vif(fit_inverse)

```
### Yes the transformation removed the flaws of the previous model. Variability of the residuals is more consistent. VIF does not change, and that's because we are only doing transformation on the y's (and hence the correlation between the x's still remains the same).

###(e)
```{r}
100/predict(fit_inverse, newdata = data.frame(cylinders = 6, horsepower= 105, weight = 3000, acceleration = 15), interval = "predict")
```
## 4.12
###(a)
```{r }
library (tidyverse)
acetylene <- 
  tribble(~x1,  ~x2,   ~x3,   ~y,
           1300, 7.5,  0.0120, 49.0,
           1300, 9,    0.0120, 50.2,
           1300, 11,   0.0115, 50.5,
           1300, 13.5, 0.0130, 48.5,
           1300, 17,   0.0135, 47.5,
           1300, 23,   0.0120, 44.5,
           1200, 5.3,  0.0400, 28.0,
           1200, 7.5,  0.0380, 31.5,
           1200, 11,   0.0320, 34.5,
           1200, 13.5, 0.0260, 35.0,
           1200, 17,   0.0340, 38.0,
           1200, 23,   0.0410, 38.5,
           1100, 5.3,  0.0840, 15.0,
           1100, 7.5,  0.0980, 17.0,
           1100, 11,   0.0920, 20.5,
           1100, 17,   0.0860, 29.5
           )

fitx1x2 <- lm (x1 ~ x2, data = acetylene)
fixx2x3 <- lm (x2 ~x3, data = acetylene)
fixx1x3 <- lm (x1~x3, data = acetylene)
plot(fitx1x2)
plot(acetylene[1:3])
```
### Yes x1 and x3 appears to be highly correlated. 

###(b)
```{r}
cor(acetylene)
sumtable <-summary(acetylene)
sumtable

#12.1 Yes I saw that the correlation between x1 and x3 is around 0.9 which is higher than 0.7,so there is multicollinearity. 
acetylene
fixmodel_old<- lm (y~ x1+x2+x3+x1*x2+x1*x3+x2*x3+I(x1^2)+ I(x2^2) + I(x3^2),acetylene)
vif(fixmodel_old)
```
### All of the VIFs are extremely high, signifying strong multicollinearity. 

### (c)
``` {r}
library("car")
acetylene["x1"] <- acetylene["x1"] - mean(acetylene$x1)
acetylene["x2"]  <-acetylene["x2"] - mean(acetylene$x2)
acetylene["x3"] <- acetylene["x3"] - mean(acetylene$x3)
fixmodel_new <- lm (y~ x1+x2+x3+x1*x2+x1*x3+x2*x3+I(x1^2)+ I(x2^2) + I(x3^2), acetylene )
vif(fixmodel_new)
```
### Yes the transformation has decreased the vif value, but the VIFs are still very high.

## 5.5 (Gas mileages of cars: Ridge and lasso regressions) Perform ridge and lasso regressions on the gas mileage data considered in Exercise 4.11 and compare the results with those of LS regression.

```{r}
library(glmnet)
mpg <- read.csv(here::here("mpg.csv"))
head(mpg)

# ridge regression
set.seed(12345)
y <- mpg$mpg
x <- model.matrix(lm(mpg~., mpg)) 
ridgefit <- cv.glmnet(x, y, alpha = 0, lambda = seq(0, 5, 0.001), nfold = 10)
lambdaridge <- ridgefit$lambda.min
lambdaridge
small.lambda.index <- which(ridgefit$lambda == ridgefit$lambda.min)
small.lambda.betas <- coef(ridgefit$glmnet.fit)[,small.lambda.index]
small.lambda.betas
r2 <- max(1 - ridgefit$cvm/var(y))
r2
# the fit is: y = -0.42x1 - 0.006x2  - 0.046x3 - 0.004x4 - 0.057x5 + 45.56. R squared is 0.7006621.

# compare with coefficients of the ls fit
coef(fit_5)
# the coefficients in the ridge regression model differ slightly from those in the original ls model. The coefficients within the ridge regression model have become more neutralized.

# lasso regression
lassofit <- cv.glmnet(x, y, alpha = 1, lambda = seq(0, 5, 0.001), nfold = 10)
lambdalasso <- lassofit$lambda.min
lambdalasso
small.lambda.index <- which(lassofit$lambda == lassofit$lambda.min)
small.lambda.betas <- coef(lassofit$glmnet.fit)[,small.lambda.index]
small.lambda.betas
r2 <- max(1 - lassofit$cvm/var(y))
r2
# the fit is: y = -0.36x1 - 0.0002x2 - 0.041x3 - 0.005x4 + 45.34. R squared is 0.6985505.
# the lasso regression dropped the acceleration term entirely and set the effect of displacement and weight to minimum.

```
## 5.6

```{r}
#acetylene = read.csv("acetylene.csv")
y = acetylene$y
x = model.matrix(y ~ ., acetylene)

# Ridge Regression
set.seed(42)
ridge_cv =  cv.glmnet(x, y, alpha = 0, lambda = seq(0, 5, 0.001), nfold = 4)
lambda_ridge = ridge_cv$lambda.min
ridge_coef = coef(ridge_cv$glmnet.fit)[,which(ridge_cv$lambda == lambda_ridge)]

# Lasso Regression
set.seed(43)
lasso_cv = cv.glmnet(x, y, alpha = 1, lambda = seq(0, 5, 0.001), nfold = 4)
lambda_lasso = lasso_cv$lambda.min
lasso_coef = coef(lasso_cv$glmnet.fit)[,which(lasso_cv$lambda == lambda_lasso)]

# LS Regression
ls_fit = lm(y ~ ., acetylene)
ls_coef = ls_fit$coefficients
```

```{r}
# Print estimated coefficients without scientific notation
# Ridge coefficients
format(ridge_coef, scientific = FALSE)

# Lasso coefficients
format(lasso_coef, scientific = FALSE)

# LS coefficients
format(ls_coef, scientific = FALSE)
```

## For the ridge regression, all estimated coefficients are smaller than their corresponding LS estimations in absolute values. In other words, the ridge regression shrinks all coefficients smoothly.

## For the lasso regression, variables x1, x3, and x1x2 have estimated coefficients of 0, meaning that they are dropped from the model.


