---
title: "401HW2"
author: 'Group #9: Qiana Yang, Dian Yu, Yijun Wu, Qiaozhen Wu'
date: "10/4/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 3.8 
#### a.
$$X = \begin{bmatrix} 1 &1\\ 1 & 2 \\1 & 3 \\ 1 & 4 \\ 1& 5\end{bmatrix}$$

#### b.
$$\begin{equation}
\begin{split}
X^{'}X 
& = \begin{bmatrix} 1 & 1 & 1 & 1 & 1 \\ 1 & 2 & 3 & 4 & 5\end{bmatrix}
\begin{bmatrix} 1 & 1\\ 1 & 2 \\1 & 3 \\ 1 & 4 \\ 1 & 5\end{bmatrix} \\
& = \begin{bmatrix} 5 & 15 \\ 15 & 55 \end{bmatrix}
\end{split}
\end{equation}$$

$$\begin{equation}
\begin{split}
(X^{'}X )^{-1}
& = \frac{1}{5\times 55-15^2}\begin{bmatrix} 55 & -15 \\ -15 & 5 \end{bmatrix}\\
& = \begin{bmatrix} 1.1 & -0.3 \\ -0.3 & 0.1 \end{bmatrix}
\end{split}
\end{equation}$$

$$\begin{equation}
\begin{split}
\begin{bmatrix} 5 & 15 \\ 15 & 55 \end{bmatrix} \begin{bmatrix} 1.1 & -0.3 \\ -0.3 & 0.1 \end{bmatrix}
& = \begin{bmatrix} (5)(1.1)+(15)(-0.3) & (5)(-0.3)+(15)(0.1) \\ (15)(1.1)+(55)(-0.3) & (15)(-0.3)+(55)(0.1) \end{bmatrix} \\
& = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}
\end{split}
\end{equation}$$

#### c.
$$\begin{equation}
\begin{split}
X^{'}Y
& = \begin{bmatrix} 1 & 1 & 1 & 1 & 1 \\ 1 & 2 & 3 & 4 & 5 \end{bmatrix} \begin{bmatrix} 2 \\ 6 \\ 7 \\ 9 \\ 10 \end{bmatrix} \\ 
& = \begin{bmatrix} 34 \\ 121 \end{bmatrix}
\end{split}
\end{equation}$$

#### d.
$$\begin{equation}
\begin{split}
\begin{bmatrix} \hat\beta_0 \\ \hat\beta_1 \end{bmatrix}
& = (X^{'}X)^{-1}XY \\
& = \begin{bmatrix} 1.1 & -0.3 \\ -0.3 & 0.1 \end{bmatrix} \begin{bmatrix} 34 \\ 121 \end{bmatrix} \\
& = \begin{bmatrix} 1.1 \\ 1.9 \end{bmatrix}
\end{split}
\end{equation}$$


## 3.9 
$\beta_0-\beta_1-\beta_2 +\beta_3= 40$

$\beta_0+\beta_1-\beta_2 - \beta_3= 40+5$

$\beta_0-\beta_1+\beta_2 - \beta_3= 40+10$

$\beta_0+\beta_1+\beta_2 + \beta_3 = 65$

$\beta_0 = 50, \beta_1 = 5, \beta_2 = 7.5, \beta_3 = 2.5$

Interpretation: $\beta_0$ could be considered as the mean salary between all combinations of demographics (white + male, non-white + male, white + female, non-white + female). The $\beta$s' in this regression model signify how much each demographic deviates from the mean when interaction is taken into account.


## 3.10 
#### a) Fit the Cobb-Douglas production function by log transformation.
```{r}

cd_data <- read.csv(here::here("Cobb-Douglas.csv"))[,-1]
head(cd_data)

cd_fit <- lm(log(output) ~ log(capital) + log(labor), cd_data)
summary(cd_fit)
```

#### b) test constant returns to scale

```{r}
beta1_hat <- coef(cd_fit)[2]
beta2_hat <- coef(cd_fit)[3]
var_cap <- vcov(cd_fit)[2,2]
var_lab <- vcov(cd_fit)[3,3]
cov_caplab <- vcov(cd_fit)[2,3]

t <- (beta1_hat + beta2_hat - 1)/sqrt(var_cap + var_lab + 2 * cov_caplab)
t

abs(t) > qt(1 - 0.05/2, df = nrow(cd_data) - 3)

# t statistic is greater than the critical value, so the result is significant. We reject the null hypothesis that beta1 + beta2 = 1.

```

#### c) alternative t test

```{r}

cd_data$alt_y <- log(cd_data$output) - log(cd_data$labor)
cd_data$alt_x1 <- log(cd_data$capital) - log(cd_data$labor)

alt_fit <- lm(alt_y ~ alt_x1 + log(labor), cd_data)

summary(alt_fit)

# the result is significant, and it is the same as that obtained in Part(b). Note that the t score obtained in Part(b) equals the t score of x2 here. The t score of x2 is significant, and therefore we reject the null hypothesis that beta1 + beta2 - 1 = 0.

```

## 3.11 
#### a) plot scatter matrix, find correlation matrix of research expenditures, number of faculty, and number of PhD students

```{r}

research <- read.csv('Research.csv')
library("PerformanceAnalytics")
research$Research <- gsub("\\$" , "", research$Research)
research$Research <- as.numeric(gsub("\\," , "", research$Research))
chart.Correlation(research[,c(2,3,5)], histogram=TRUE, pch=19)
cor(research[,c(2,3,5)])

# the number of PhD students is highly correlated with the number of faculty, with r = 0.90. Research expenditure is correlated with the number of PhD students and faculty with r = 0.82 and r = 0.76 respectively.

```
#### b) fit a regression model for Research versus Faculty and PhD.

```{r}

res_fit <- lm(Research ~ Faculty + PhD, research)
summary(res_fit)
head(research)

try1 <- lm(Research ~ Faculty, research)
summary(try1)

try2 <- lm(Research ~ PhD, research)
summary(try2)

# The number of faculty is highly correlated with the number of students (r = 0.9). In this case, research expenditures are also calculated by weighing both total research expenditures and research expenditures by faculty. Given faculty with more research grant funds more students and the number of faculty can be estimated by total research expenditures / research expenditures by faculty, the relationship between faculty and student is not independent, and therefore the model we have here has a multicollinearity problem.

```

## 3.12

```{r}

R <- rbind(c(1, 0.5), c(0.5, 1))
r <- c(0.4, 0.8)
beta_hat_star <- solve(R)%*%r

#standardized regression coefficients:
beta_hat_star
```

```{r}
beta_1 = beta_hat_star[1, 1]*5/2
beta_2 = beta_hat_star[2, 1]*5/4
beta_0 = 10-(beta_1*3+beta_2*5)
beta_unstand <- c("beta_0" = beta_0, "beta_1" = beta_1, "beta_2" = beta_2)
beta_unstand
```
The regression model is $y = x_2 + 5$

## 3.14
#### a) Fit prediction model - set male and purchase as the reference variables

```{r}

salaries <- read.csv(here::here("salaries.csv"), stringsAsFactors = T)

head(salaries)

salaries$Gender <- relevel(salaries$Gender, "Male")
salaries$Dept <- relevel(salaries$Dept, "Purchase")
salfit <- lm(log10(Salary)~YrsEm + PriorYr + Education + Super + Gender + Dept, salaries)

summary(salfit)

```
#### b) set female and sales as the new reference categories

```{r}

salaries$Gender <- relevel(salaries$Gender, "Female")
salaries$Dept <- relevel(salaries$Dept, "Sales")
salfit2 <- lm(log10(Salary)~YrsEm + PriorYr + Education + Super + Gender + Dept, salaries)

summary(salfit2)

# coefficient for male is -0.023, opposite to what we got for female in a). 
# coefficients for Purchase, Advertising, and Engineering are 0.0938, 0.0550, 0.0880 respectively.  

```
#### c) predict a person's salary based on criteria below

```{r}

predict1 <- predict(salfit, newdata = data.frame(YrsEm = 8, PriorYr = 10, Education = 12, Gender = "Male", Dept = "Sales", Super = 5), interval = "prediction")

10^c(predict1)

# the salary would be $41500.04, with lower bound $31937.20 and upper bound $53926.24.

```

#### d) The coefficient of Engg is highly nonsignificant with a P-value = 0.774 in the above regression. But if Sales is used as the reference category, the coefficient of Engg is highly significant with a P-value < 0:001. Interpret this result.

Answer: the salaries of those from the Engineering Dept are not significantly different from those from the Purchase Dept, but they are significantly different from those from the Sales Dept.

#### e) In the above model, the coefficient of Female is nonsignificant with P = 0.115, so there is not a significant difference between the salaries of Males and Females, controlling for other variables. Should we drop Gender as a predictor variable? For Dept the question is more complicated since the coefficients of Advert and Engg are nonsignificant but the coefficient of Sales in highly significant with P = 0.0002. Should the nonsignificant categories, Advert and Engg, be pooled with the reference category, Purchase, or should the categories be left unpooled or should the Dept be dropped as a predictor variable?

Answer: We may drop gender as a predictor variable because there is no significant difference between the two genders. Since the coefficient of Sales is significant, we should not pool the nonsignificant categories with the reference category, because otherwise we would lose information when changing reference categories. Dept should not be dropped as a predictor variable, because at least one of the dummy variables is significant.